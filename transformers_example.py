import torch
import torch.nn as nn
import torch.nn.functional as F
import math

# ======================================================================================
# 1. ç¯å¢ƒè®¾ç½®ï¼šæ¨¡æ‹Ÿä¸€ä¸ªå¾®å‹GPTæ¨¡å‹
# ä¸ºäº†æ¸…æ™°åœ°å±•ç¤ºè®¡ç®—è¿‡ç¨‹ï¼Œæˆ‘ä»¬ä¸ç›´æ¥åŠ è½½2Bå‚æ•°çš„æ¨¡å‹ï¼Œè€Œæ˜¯æ„å»ºä¸€ä¸ªç»“æ„ç›¸åŒä½†å°ºå¯¸æå°çš„ç‰ˆæœ¬ã€‚
# è¿™èƒ½è®©æˆ‘ä»¬è½»æ¾æŸ¥çœ‹æ¯ä¸€æ­¥çš„å¼ é‡ï¼ˆå‘é‡ï¼‰å˜åŒ–ã€‚
# ======================================================================================

# é…ç½®ä¸ `å‘é‡çš„æ·±åº¦ä¹‹æ—…` ç« èŠ‚ä¸­çš„ç¤ºä¾‹ä¿æŒä¸€è‡´
d_model = 4  # å‘é‡ç»´åº¦
vocab_size = 10  # å‡è®¾æˆ‘ä»¬çš„è¯æ±‡è¡¨å¾ˆå°
sequence_length = 2  # è¾“å…¥åºåˆ—é•¿åº¦ "an apple"

# å®šä¹‰ä¸€ä¸ªç®€åŒ–çš„ã€å•å±‚çš„Transformerè§£ç å™¨å—
class TinyDecoderBlock(nn.Module):
    def __init__(self, d_model):
        super().__init__()
        # æ³¨æ„åŠ›æœºåˆ¶çš„æƒé‡çŸ©é˜µ
        self.W_q = nn.Linear(d_model, d_model, bias=False)
        self.W_k = nn.Linear(d_model, d_model, bias=False)
        self.W_v = nn.Linear(d_model, d_model, bias=False)

        # å‰é¦ˆç½‘ç»œ (Feed-Forward Network)
        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_model * 4),
            nn.ReLU(),
            nn.Linear(d_model * 4, d_model)
        )

        # å±‚å½’ä¸€åŒ–
        self.ln1 = nn.LayerNorm(d_model)
        self.ln2 = nn.LayerNorm(d_model)

    def forward(self, x):
        # æ®‹å·®è¿æ¥ 1
        residual = x
        x = self.ln1(x)

        # æ­¥éª¤ 2: QKV çº¿æ€§å˜æ¢
        q = self.W_q(x)
        k = self.W_k(x)
        v = self.W_v(x)

        # æ­¥éª¤ 3: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        attn_scores = q @ k.transpose(-2, -1) / math.sqrt(d_model)

        # åˆ›å»ºä¸€ä¸ªä¸Šä¸‰è§’æ©ç ï¼Œé˜²æ­¢çœ‹åˆ°æœªæ¥çš„token
        mask = torch.triu(torch.ones(attn_scores.size(-2), attn_scores.size(-1)), diagonal=1).bool()
        attn_scores = attn_scores.masked_fill(mask, -float('inf'))

        # æ­¥éª¤ 4: Softmax å½’ä¸€åŒ–
        attn_weights = F.softmax(attn_scores, dim=-1)

        # æ­¥éª¤ 5: ç”Ÿæˆä¸Šä¸‹æ–‡å‘é‡
        context_vector = attn_weights @ v

        # åº”ç”¨æ®‹å·®è¿æ¥
        x = residual + context_vector

        # æ®‹å·®è¿æ¥ 2
        residual = x
        x = self.ln2(x)

        # æ­¥éª¤ 6: FFN éçº¿æ€§å˜æ¢
        ffn_output = self.ffn(x)

        # åº”ç”¨æ®‹å·®è¿æ¥
        x = residual + ffn_output

        return x, q, k, v, attn_scores, attn_weights, context_vector

# æ¨¡æ‹Ÿä¸€ä¸ªå®Œæ•´çš„å¾®å‹GPT
class TinyGPT(nn.Module):
    def __init__(self, vocab_size, d_model):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.decoder_block = TinyDecoderBlock(d_model)
        self.output_head = nn.Linear(d_model, vocab_size)

    def forward(self, idx):
        # æ­¥éª¤ 1: åˆå§‹å‘é‡ (Embeddings)
        token_embeddings = self.embedding(idx)
        # (åœ¨çœŸå®æ¨¡å‹ä¸­ï¼Œè¿™é‡Œè¿˜ä¼šåŠ ä¸Šä½ç½®ç¼–ç )

        # é€šè¿‡è§£ç å™¨å±‚å¤„ç†
        final_vectors, q, k, v, scores, weights, context = self.decoder_block(token_embeddings)

        # æ­¥éª¤ 7 (éƒ¨åˆ†): ç”Ÿæˆé¢„æµ‹
        logits = self.output_head(final_vectors)

        return logits, token_embeddings, q, k, v, scores, weights, context

# ä¸ºäº†ç»“æœå¯å¤ç°ï¼Œè®¾ç½®éšæœºç§å­
torch.manual_seed(42)

# ======================================================================================
# 2. è®­ç»ƒæ¼”ç¤ºï¼šä¸¥æ ¼å‚ç…§ `å‘é‡çš„æ·±åº¦ä¹‹æ—…` ç« èŠ‚é¡ºåº
# ======================================================================================
print("="*50)
print("ğŸš€ å¼€å§‹å‘é‡çš„æ·±åº¦ä¹‹æ—…ï¼šä¸€æ¬¡å®Œæ•´çš„è®­ç»ƒè¿­ä»£")
print("="*50)

# å®ä¾‹åŒ–æ¨¡å‹
model = TinyGPT(vocab_size, d_model)

# å‡†å¤‡è¾“å…¥æ•°æ®
# å‡è®¾ "an" -> 0, "apple" -> 1, "a" -> 2
input_tokens = torch.tensor([[0, 1]])  # è¾“å…¥: "an apple"
target_token_for_apple = torch.tensor([2]) # å½“è¾“å…¥æ˜¯ "an apple" æ—¶ï¼Œæˆ‘ä»¬å¸Œæœ›æ¨¡å‹åœ¨ "apple" çš„ä½ç½®ä¸Šé¢„æµ‹å‡º "a"

# --- å‰å‘ä¼ æ’­ (Forward Pass) ---
logits, embeddings, q, k, v, scores, weights, context = model(input_tokens)

print("
[ç¬¬ä¸€æ­¥ï¼šåˆå§‹å‘é‡ (Embeddings)]")
print("æ¨¡å‹æ¥æ”¶åˆ° 'an apple' (token ID: [0, 1])ï¼Œå¹¶æŸ¥æ‰¾å®ƒä»¬çš„åˆå§‹å‘é‡ã€‚")
print(f"åˆå§‹å‘é‡ (Embeddings):
{embeddings.detach()}
")

print("
[ç¬¬äºŒæ­¥ï¼šQKVçº¿æ€§å˜æ¢]")
print("æ¯ä¸ªè¾“å…¥å‘é‡åˆ†åˆ«ä¸ W_q, W_k, W_v çŸ©é˜µç›¸ä¹˜ï¼Œç”Ÿæˆ Query, Key, Value å‘é‡ã€‚")
print(f"Query (æŸ¥è¯¢) å‘é‡:
{q.detach()}
")
print(f"Key (é”®) å‘é‡:
{k.detach()}
")
print(f"Value (å€¼) å‘é‡:
{v.detach()}
")

print("
[ç¬¬ä¸‰æ­¥ï¼šè®¡ç®—æ³¨æ„åŠ›åˆ†æ•°]")
print("ç”¨æ¯ä¸ªä½ç½®çš„ Query å‘é‡å»å’Œå®ƒèƒ½çœ‹åˆ°çš„æ‰€æœ‰ä½ç½®çš„ Key å‘é‡åšç‚¹ç§¯ï¼Œå¹¶è¿›è¡Œç¼©æ”¾ã€‚")
print("ç”±äºæœ‰æ©ç ï¼Œ'apple' (ä½ç½®1) åªèƒ½å…³æ³¨ 'an' (ä½ç½®0) å’Œå®ƒè‡ªèº«ã€‚")
print(f"åŸå§‹æ³¨æ„åŠ›åˆ†æ•° (åº”ç”¨æ©ç å):
{scores.detach()}
")

print("
[ç¬¬å››æ­¥ï¼šSoftmaxå½’ä¸€åŒ–]")
print("å°†åˆ†æ•°è½¬æ¢ä¸º0åˆ°1ä¹‹é—´ã€æ€»å’Œä¸º1çš„æƒé‡ï¼Œä»£è¡¨æ³¨æ„åŠ›åˆ†å¸ƒã€‚")
print(f"æ³¨æ„åŠ›æƒé‡ (Attention Weights):
{weights.detach()}
")
print("è§£è¯»: åœ¨å¤„ç† 'apple' è¿™ä¸ªè¯æ—¶ï¼Œæ¨¡å‹å°† ~14% çš„æ³¨æ„åŠ›æ”¾åœ¨ 'an' ä¸Šï¼Œ~86% çš„æ³¨æ„åŠ›æ”¾åœ¨ 'apple' è‡ªèº«ã€‚")

print("
[ç¬¬äº”æ­¥ï¼šç”Ÿæˆä¸Šä¸‹æ–‡å‘é‡]")
print("ç”¨æ³¨æ„åŠ›æƒé‡å¯¹æ‰€æœ‰ Value å‘é‡è¿›è¡ŒåŠ æƒæ±‚å’Œï¼Œå¾—åˆ°èåˆäº†ä¸Šä¸‹æ–‡çš„æ–°å‘é‡ã€‚")
print(f"ä¸Šä¸‹æ–‡å‘é‡ (Context Vector):
{context.detach()}
")

print("
[ç¬¬å…­æ­¥ï¼šFFN, æ®‹å·®è¿æ¥ & å±‚å½’ä¸€åŒ–]")
print("å‘é‡ç»è¿‡å‰é¦ˆç½‘ç»œã€æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–ï¼Œè¿›è¡Œæ·±åº¦åŠ å·¥ï¼Œå¾—åˆ°æœ€ç»ˆè¾“å‡ºå‘é‡ã€‚")
final_vector_for_apple = logits[:, 1, :] # æˆ‘ä»¬åªå…³å¿ƒåœ¨ "apple" ä½ç½®çš„è¾“å‡º
print(f"åœ¨ 'apple' ä½ç½®çš„æœ€ç»ˆè¾“å‡ºå‘é‡:
{final_vector_for_apple.detach()}
")

print("
[ç¬¬ä¸ƒæ­¥ï¼šæŸå¤±å‡½æ•°è®¡ç®—]")
print("æ¨¡å‹ä½¿ç”¨æœ€ç»ˆå‘é‡é¢„æµ‹è¯æ±‡è¡¨ä¸­æ¯ä¸ªè¯çš„æ¦‚ç‡ï¼Œå¹¶ä¸çœŸå®ç›®æ ‡ 'a' (token ID: 2) å¯¹æ¯”ã€‚")
# æˆ‘ä»¬åªå…³å¿ƒåœ¨ "apple" ä½ç½®çš„é¢„æµ‹ï¼Œå› ä¸ºè¿™æ˜¯æˆ‘ä»¬æœ‰ç­”æ¡ˆçš„åœ°æ–¹
logits_for_apple = logits[:, 1, :]
loss = F.cross_entropy(logits_for_apple, target_token_for_apple)
print(f"æ¨¡å‹é¢„æµ‹çš„Logits (åœ¨ 'apple' ä½ç½®):
{logits_for_apple.detach()}
")
print(f"çœŸå®ç›®æ ‡ Token ID: {target_token_for_apple.item()}")
print(f"è®¡ç®—å‡ºçš„äº¤å‰ç†µæŸå¤± (Cross-Entropy Loss): {loss.item():.4f}
")
print("è§£è¯»: æŸå¤±å€¼æ˜¯ä¸€ä¸ªè¡¡é‡ 'é¢„æµ‹é”™è¯¯ç¨‹åº¦' çš„æ ‡é‡ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯é€šè¿‡è°ƒæ•´æƒé‡æ¥è®©å®ƒå˜å°ã€‚")

# --- åå‘ä¼ æ’­ (Backward Pass) ---

# æ¸…ç©ºæ—§çš„æ¢¯åº¦
model.zero_grad()

print("
[ç¬¬å…«æ­¥ï¼šåå‘ä¼ æ’­ä¸æ¢¯åº¦è®¡ç®—]")
print("æŸå¤±å€¼å¼€å§‹åå‘ä¼ æ’­ï¼Œä½¿ç”¨é“¾å¼æ³•åˆ™è®¡ç®—å‡ºæ¯ä¸ªæƒé‡ç›¸å¯¹äºæŸå¤±çš„æ¢¯åº¦ã€‚")
loss.backward()
print("æ¢¯åº¦è®¡ç®—å®Œæˆã€‚æ¢¯åº¦æŒ‡æ˜äº†æ¯ä¸ªæƒé‡åº”è¯¥è°ƒæ•´çš„æ–¹å‘å’Œå¹…åº¦ã€‚
")

print("
[ç¬¬ä¹æ­¥ï¼šæƒé‡æ›´æ–°ä¸ä¼˜åŒ–]")
print("ä¼˜åŒ–å™¨ (å¦‚Adam) ä½¿ç”¨è®¡ç®—å‡ºçš„æ¢¯åº¦æ¥æ›´æ–°æ¨¡å‹çš„æ¯ä¸€ä¸ªæƒé‡ã€‚")
# è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„æ‰‹åŠ¨æ›´æ–°è¿‡ç¨‹ï¼Œç”¨ä»¥æ¼”ç¤º
learning_rate = 0.01
print(f"ä»¥ 'æ³¨æ„åŠ›å±‚' çš„ W_q æƒé‡ä¸ºä¾‹ (å­¦ä¹ ç‡: {learning_rate}):")
wq_weight_before = model.decoder_block.W_q.weight.data.clone()
print(f"æ›´æ–°å‰çš„ W_q æƒé‡ (éƒ¨åˆ†):
{wq_weight_before[0, :].numpy()}...")

# æ‰‹åŠ¨æ‰§è¡Œä¸€æ­¥æ¢¯åº¦ä¸‹é™
with torch.no_grad():
    for param in model.parameters():
        if param.grad is not None:
            param -= learning_rate * param.grad

wq_weight_after = model.decoder_block.W_q.weight.data.clone()
print(f"
æ›´æ–°åçš„ W_q æƒé‡ (éƒ¨åˆ†):
{wq_weight_after[0, :].numpy()}...")
print("å¯ä»¥çœ‹åˆ°æƒé‡å·²ç»è¢«å¾®è°ƒã€‚è¿™ä¸ªè¿‡ç¨‹ä¼šåœ¨æ•°ç™¾ä¸‡ä¸ªæ ·æœ¬ä¸Šé‡å¤æ•°åäº¿æ¬¡ï¼Œæ¨¡å‹çš„èƒ½åŠ›ä»è€Œå¾—åˆ°æå‡ã€‚")

print("
" + "="*50)
print("ğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
print("="*50)
